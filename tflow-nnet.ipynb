{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, cross_validation\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.inning /= 9.0\n",
    "data.order /= 9.0\n",
    "data.weekday /= 7.0\n",
    "data.month /= 12.0\n",
    "data.balls /= 4.0\n",
    "data.strikes /= 3.0\n",
    "sample = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_features = ['pitcher_id', 'batter_id', 'away_team', 'home_team', \n",
    "                'year', 'b_stand', 'p_throws', 'inning_half',  \n",
    "                'batter_team', 'pitcher_team', 'type']\n",
    "real_features = ['night', 'inning', 'order', 'home', 'weekday', \n",
    "                 'month', 'balls', 'strikes', 'sz_top', 'sz_bot']\n",
    "n_cat = len(cat_features)\n",
    "n_real = len(real_features)\n",
    "ptypes = sample.type.unique()\n",
    "depths = [len(data[col].unique()) for col in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = len(sample)\n",
    "n_outputs = len(ptypes)\n",
    "embedding_size = 30\n",
    "prep = learn.preprocessing.CategoricalProcessor()\n",
    "cat_data = np.array(list(prep.fit_transform(sample[cat_features])))\n",
    "n_pitchers = cat_data[:,0].max() + 1\n",
    "n_batters = cat_data[:,1].max() + 1\n",
    "\n",
    "real_data = sample[real_features].values\n",
    "loc_data = sample[['px','pz']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_type():\n",
    "    p_embeddings = tf.Variable(tf.random_uniform([n_pitchers, embedding_size], -1.0, 1.0))\n",
    "    b_embeddings = tf.Variable(tf.random_uniform([n_batters, embedding_size], -1.0, 1.0))\n",
    "\n",
    "    cat_batch = tf.placeholder(tf.int32, [None, n_cat-1])\n",
    "    real_batch = tf.placeholder(tf.float32, [None, n_real])\n",
    "    result_batch = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    pitchers = cat_batch[:,0]\n",
    "    batters = cat_batch[:,1]\n",
    "\n",
    "    p_embed = tf.nn.embedding_lookup(p_embeddings, pitchers)\n",
    "    b_embed = tf.nn.embedding_lookup(b_embeddings, batters)\n",
    "\n",
    "    inputs = [p_embed, b_embed, real_batch]\n",
    "    for i in range(2, n_cat-1):\n",
    "        inputs.append(tf.one_hot(cat_batch[:,i], depth=depths[i],\n",
    "                      on_value=1.0, off_value=0.0, dtype=tf.float32))\n",
    "    input_layer = tf.concat(concat_dim=1, values=inputs)\n",
    "\n",
    "    in_dim = input_layer.get_shape()[1]\n",
    "\n",
    "    W1 = tf.Variable(tf.zeros([in_dim, 75]))\n",
    "    b1 = tf.Variable(tf.zeros([75]))\n",
    "\n",
    "    hidden = tf.nn.relu(tf.matmul(input_layer, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.zeros([75, n_outputs]))\n",
    "    b2 = tf.Variable(tf.zeros([n_outputs]))\n",
    "\n",
    "    y = tf.nn.softmax(tf.matmul(hidden, W2) + b2)\n",
    "    y_ = tf.one_hot(indices=result_batch - 1, depth=n_outputs, \n",
    "                    on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    batch_size = 500\n",
    "    for i in range(5000):\n",
    "        idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "        input_data = {cat_batch : cat_data[idx,:-1],\n",
    "                                real_batch : real_data[idx], \n",
    "                                result_batch : cat_data[idx,-1]}\n",
    "        sess.run(train_step, feed_dict=input_data)\n",
    "        if i % 500 == 0:\n",
    "            print(sess.run(cross_entropy, feed_dict=input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_loc():\n",
    "    n_mixtures = 4\n",
    "    p_embeddings = tf.Variable(tf.random_uniform([n_pitchers, embedding_size], -1.0, 1.0))\n",
    "    b_embeddings = tf.Variable(tf.random_uniform([n_batters, embedding_size], -1.0, 1.0))\n",
    "\n",
    "    cat_batch = tf.placeholder(tf.int32, [None, n_cat])\n",
    "    real_batch = tf.placeholder(tf.float32, [None, n_real])\n",
    "    loc_batch = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "    n_items = tf.shape(cat_batch)[0]\n",
    "\n",
    "    pitchers = cat_batch[:,0]\n",
    "    batters = cat_batch[:,1]\n",
    "\n",
    "    p_embed = tf.nn.embedding_lookup(p_embeddings, pitchers)\n",
    "    b_embed = tf.nn.embedding_lookup(b_embeddings, batters)\n",
    "\n",
    "    inputs = [p_embed, b_embed, real_batch]\n",
    "    for i in range(2, n_cat):\n",
    "        inputs.append(tf.one_hot(cat_batch[:,i], depth=depths[i],\n",
    "                      on_value=1.0, off_value=0.0, dtype=tf.float32))\n",
    "    input_layer = tf.concat(concat_dim=1, values=inputs)\n",
    "\n",
    "    in_dim = input_layer.get_shape()[1]\n",
    "\n",
    "    W1 = tf.Variable(tf.zeros([in_dim, 75]))\n",
    "    b1 = tf.Variable(tf.random_normal([75], 0.0, .01)) + 0.03\n",
    "\n",
    "    hidden = tf.nn.relu(tf.matmul(input_layer, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([75, n_mixtures], 0.0, .01))\n",
    "    b2 = tf.Variable(tf.random_normal([n_mixtures], 0.0, .01))\n",
    "    W3 = tf.Variable(tf.random_normal([75, n_mixtures*2], 0.0, .01))\n",
    "    b3 = tf.Variable(tf.random_normal([n_mixtures*2], 0.0, .01))\n",
    "    W4 = tf.Variable(tf.random_normal([75, n_mixtures*2], 0.0, .01))\n",
    "    b4 = tf.Variable(tf.random_normal([n_mixtures*2], 0.0, .01))\n",
    "\n",
    "    weights = tf.nn.softmax(tf.matmul(hidden, W2) + b2)\n",
    "    means =  tf.reshape(tf.matmul(hidden, W3) + b3, tf.pack([n_items, n_mixtures, 2]))\n",
    "    stdevs = tf.reshape(tf.exp(tf.matmul(hidden, W4) + b4), tf.pack([n_items, n_mixtures, 2]))\n",
    "\n",
    "    def likelihood(i):\n",
    "        normals = tf.contrib.distributions.MultivariateNormalDiag(means[i], stdevs[i])\n",
    "        return tf.reduce_sum(weights[i] * normals.pdf(loc_batch[i]))\n",
    "\n",
    "    likelihoods = tf.map_fn(likelihood, tf.range(0, n_items), dtype=tf.float32)\n",
    "    loglike = tf.reduce_mean(tf.log(likelihoods))\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(0.0005).minimize(-loglike)\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    \n",
    "    batch_size = 500\n",
    "    for i in range(1000):\n",
    "        idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "        input_data = feed_dict={cat_batch : cat_data[idx],\n",
    "                                real_batch : real_data[idx], \n",
    "                                loc_batch : loc_data[idx]}\n",
    "        if i % 1 == 0:\n",
    "            print(sess.run(loglike, feed_dict = input_data)) \n",
    "        sess.run(train_step, feed_dict = input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learn_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.23907\n",
      "-5.03791\n",
      "-5.15264\n",
      "-5.10832\n",
      "-5.04511\n",
      "-4.96719\n",
      "-4.97253\n",
      "-4.80116\n",
      "-4.65361\n",
      "-4.8225\n",
      "-4.6065\n",
      "-4.64121\n",
      "-4.64014\n",
      "-4.43933\n",
      "-4.26636\n",
      "-4.28168\n",
      "-4.11485\n",
      "-4.14881\n",
      "-4.1263\n",
      "-4.08372\n",
      "-3.97546\n",
      "-3.86109\n",
      "-3.74915\n",
      "-3.64913\n",
      "-3.72216\n",
      "-3.62633\n",
      "-3.55088\n",
      "-3.56211\n",
      "-3.54661\n",
      "-3.49627\n",
      "-3.45378\n",
      "-3.43194\n",
      "-3.33364\n",
      "-3.43077\n",
      "-3.44046\n",
      "-3.34775\n",
      "-3.32243\n",
      "-3.30961\n",
      "-3.36588\n",
      "-3.34463\n",
      "-3.31281\n",
      "-3.35469\n",
      "-3.31403\n",
      "-3.23277\n",
      "-3.24329\n",
      "-3.31123\n",
      "-3.32127\n",
      "-3.2536\n",
      "-3.24097\n",
      "-3.28541\n",
      "-3.29158\n",
      "-3.24402\n",
      "-3.26904\n",
      "-3.21895\n",
      "-3.21912\n",
      "-3.23158\n",
      "-3.22876\n",
      "-3.18937\n",
      "-3.14893\n",
      "-3.19317\n",
      "-3.14052\n",
      "-3.14486\n",
      "-3.09174\n",
      "-3.12899\n",
      "-3.12544\n",
      "-3.0885\n",
      "-3.06068\n",
      "-3.09494\n",
      "-3.03889\n",
      "-3.0687\n",
      "-2.97176\n",
      "-3.02867\n",
      "-2.9992\n",
      "-2.97577\n",
      "-3.02054\n",
      "-2.94489\n",
      "-2.89979\n",
      "-2.88469\n",
      "-2.84248\n",
      "-2.86562\n",
      "-2.86496\n",
      "-2.88103\n",
      "-2.81906\n",
      "-2.79203\n",
      "-2.73022\n",
      "-2.75945\n",
      "-2.69328\n",
      "-2.68007\n",
      "-2.67807\n",
      "-2.66063\n",
      "-2.59185\n",
      "-2.66243\n",
      "-2.67212\n",
      "-2.65078\n",
      "-2.78911\n",
      "-2.67577\n",
      "-2.67409\n",
      "-2.64795\n",
      "-2.74988\n",
      "-2.72463\n",
      "-2.62984\n",
      "-2.67303\n",
      "-2.57196\n",
      "-2.54297\n",
      "-2.61875\n",
      "-2.62585\n",
      "-2.70295\n",
      "-2.56444\n",
      "-2.70178\n",
      "-2.58247\n",
      "-2.63766\n",
      "-2.62987\n",
      "-2.65167\n",
      "-2.66492\n",
      "-2.61924\n",
      "-2.62553\n",
      "-2.55787\n",
      "-2.68428\n",
      "-2.61227\n",
      "-2.59717\n",
      "-2.59987\n",
      "-2.63635\n",
      "-2.63331\n",
      "-2.67173\n",
      "-2.62681\n",
      "-2.6824\n",
      "-2.58496\n",
      "-2.62815\n",
      "-2.64586\n",
      "-2.65647\n",
      "-2.67783\n",
      "-2.60071\n",
      "-2.62405\n",
      "-2.6496\n",
      "-2.60744\n",
      "-2.68544\n",
      "-2.60015\n",
      "-2.57972\n",
      "-2.5979\n",
      "-2.70344\n",
      "-2.64385\n",
      "-2.60976\n",
      "-2.58161\n",
      "-2.71707\n",
      "-2.60575\n",
      "-2.64263\n",
      "-2.63089\n",
      "-2.59827\n",
      "-2.58828\n",
      "-2.6318\n",
      "-2.66324\n",
      "-2.65154\n",
      "-2.64473\n",
      "-2.59503\n",
      "-2.52186\n",
      "-2.60329\n",
      "-2.61825\n",
      "-2.59328\n",
      "-2.64509\n",
      "-2.65967\n",
      "-2.55419\n",
      "-2.50726\n",
      "-2.73814\n",
      "-2.62664\n",
      "-2.6122\n",
      "-2.52824\n",
      "-2.68335\n",
      "-2.58891\n",
      "-2.62472\n",
      "-2.55867\n",
      "-2.67824\n",
      "-2.65452\n",
      "-2.58125\n",
      "-2.61847\n",
      "-2.71554\n",
      "-2.62636\n",
      "-2.60822\n",
      "-2.61332\n",
      "-2.61511\n",
      "-2.54013\n",
      "-2.73972\n",
      "-2.5604\n",
      "-2.68581\n",
      "-2.6112\n",
      "-2.62007\n",
      "-2.72111\n",
      "-2.67678\n",
      "-2.66582\n",
      "-2.59985\n",
      "-2.70969\n",
      "-2.64311\n",
      "-2.61375\n",
      "-2.65343\n",
      "-2.60364\n",
      "-2.61244\n",
      "-2.62715\n",
      "-2.63519\n",
      "-2.5723\n",
      "-2.5459\n",
      "-2.6143\n",
      "-2.59253\n",
      "-2.63119\n",
      "-2.59763\n",
      "-2.58375\n",
      "-2.60766\n",
      "-2.50475\n",
      "-2.58913\n",
      "-2.56948\n",
      "-2.56644\n",
      "-2.50476\n",
      "-2.53605\n",
      "-2.56434\n",
      "-2.57204\n",
      "-2.65364\n",
      "-2.63191\n",
      "-2.52298\n",
      "-2.59652\n",
      "-2.60186\n",
      "-2.54555\n",
      "-2.75405\n",
      "-2.64885\n",
      "-2.63452\n",
      "-2.57986\n",
      "-2.53428\n",
      "-2.56689\n",
      "-2.52868\n",
      "-2.53673\n",
      "-2.57267\n",
      "-2.53935\n",
      "-2.57748\n",
      "-2.57221\n",
      "-2.57861\n",
      "-2.58582\n",
      "-2.53836\n",
      "-2.59105\n",
      "-2.51002\n",
      "-2.65186\n",
      "-2.59174\n",
      "-2.51979\n",
      "-2.65145\n",
      "-2.55118\n",
      "-2.61723\n",
      "-2.5558\n",
      "-2.58301\n",
      "-2.61297\n",
      "-2.61656\n",
      "-2.53764\n",
      "-2.67065\n",
      "-2.64783\n",
      "-2.6672\n",
      "-2.54827\n",
      "-2.52452\n",
      "-2.60209\n",
      "-2.62206\n",
      "-2.58229\n",
      "-2.48177\n",
      "-2.59226\n",
      "-2.52074\n",
      "-2.69413\n",
      "-2.50144\n",
      "-2.57113\n",
      "-2.511\n",
      "-2.59712\n",
      "-2.60738\n",
      "-2.61402\n",
      "-2.61398\n",
      "-2.57754\n",
      "-2.63303\n",
      "-2.57857\n",
      "-2.60094\n",
      "-2.68917\n",
      "-2.67903\n",
      "-2.55694\n",
      "-2.52719\n",
      "-2.58866\n",
      "-2.68717\n",
      "-2.53356\n",
      "-2.58078\n",
      "-2.53977\n",
      "-2.56263\n",
      "-2.57303\n",
      "-2.52364\n",
      "-2.56397\n",
      "-2.53387\n",
      "-2.61395\n",
      "-2.57092\n",
      "-2.61633\n",
      "-2.62372\n",
      "-2.60059\n",
      "-2.52812\n",
      "-2.4892\n",
      "-2.5409\n",
      "-2.54497\n",
      "-2.52741\n",
      "-2.58682\n",
      "-2.52039\n",
      "-2.48511\n",
      "-2.48924\n",
      "-2.63351\n",
      "-2.52532\n",
      "-2.55191\n",
      "-2.57931\n",
      "-2.56765\n",
      "-2.5872\n",
      "-2.52794\n",
      "-2.60032\n",
      "-2.58154\n",
      "-2.60339\n",
      "-2.65436\n",
      "-2.54975\n",
      "-2.50909\n",
      "-2.55043\n",
      "-2.53867\n",
      "-2.56562\n",
      "-2.52416\n",
      "-2.50354\n",
      "-2.49299\n",
      "-2.58042\n",
      "-2.52954\n",
      "-2.51014\n",
      "-2.53473\n",
      "-2.58319\n",
      "-2.60556\n",
      "-2.48592\n",
      "-2.56005\n",
      "-2.62842\n",
      "-2.51698\n",
      "-2.66199\n",
      "-2.54542\n",
      "-2.55008\n",
      "-2.56292\n",
      "-2.58019\n",
      "-2.56128\n",
      "-2.55962\n",
      "-2.58863\n",
      "-2.57685\n",
      "-2.55595\n",
      "-2.55183\n",
      "-2.58986\n",
      "-2.51902\n",
      "-2.55178\n",
      "-2.58311\n",
      "-2.60121\n",
      "-2.54705\n",
      "-2.51333\n",
      "-2.59516\n",
      "-2.59411\n",
      "-2.63291\n",
      "-2.53164\n",
      "-2.55726\n",
      "-2.4695\n",
      "-2.47389\n",
      "-2.53786\n",
      "-2.55338\n",
      "-2.55576\n",
      "-2.5552\n",
      "-2.52731\n",
      "-2.52662\n",
      "-2.4649\n",
      "-2.52987\n",
      "-2.6214\n",
      "-2.51454\n",
      "-2.51111\n",
      "-2.54597\n",
      "-2.52846\n",
      "-2.56359\n",
      "-2.55751\n",
      "-2.61993\n",
      "-2.59027\n",
      "-2.60385\n",
      "-2.56367\n",
      "-2.47454\n",
      "-2.46079\n",
      "-2.56376\n",
      "-2.50271\n",
      "-2.51455\n",
      "-2.58474\n",
      "-2.53465\n",
      "-2.4806\n",
      "-2.44893\n",
      "-2.49323\n",
      "-2.56291\n",
      "-2.50285\n",
      "-2.44255\n",
      "-2.55407\n",
      "-2.51634\n",
      "-2.64024\n",
      "-2.52307\n",
      "-2.49401\n",
      "-2.50299\n",
      "-2.51535\n",
      "-2.60199\n",
      "-2.57212\n",
      "-2.57168\n",
      "-2.51099\n",
      "-2.63665\n",
      "-2.60198\n",
      "-2.6303\n",
      "-2.59175\n",
      "-2.53628\n",
      "-2.68905\n",
      "-2.58657\n",
      "-2.51178\n",
      "-2.49755\n",
      "-2.5769\n",
      "-2.56059\n",
      "-2.57946\n",
      "-2.53464\n",
      "-2.59536\n",
      "-2.45891\n",
      "-2.56475\n",
      "-2.60313\n",
      "-2.63545\n",
      "-2.56422\n",
      "-2.48719\n",
      "-2.54085\n",
      "-2.58996\n",
      "-2.48765\n",
      "-2.48311\n",
      "-2.51837\n",
      "-2.64978\n",
      "-2.56511\n",
      "-2.57742\n",
      "-2.546\n",
      "-2.54411\n",
      "-2.65857\n",
      "-2.49666\n",
      "-2.54823\n",
      "-2.52936\n",
      "-2.59717\n",
      "-2.53017\n",
      "-2.56249\n",
      "-2.51151\n",
      "-2.57156\n",
      "-2.50981\n",
      "-2.5303\n",
      "-2.52705\n",
      "-2.47871\n",
      "-2.54752\n",
      "-2.56692\n",
      "-2.52913\n",
      "-2.4915\n",
      "-2.52442\n",
      "-2.48065\n",
      "-2.49116\n",
      "-2.52855\n",
      "-2.55509\n",
      "-2.55634\n",
      "-2.5935\n",
      "-2.53164\n",
      "-2.51771\n",
      "-2.55173\n",
      "-2.51472\n",
      "-2.43335\n",
      "-2.45838\n",
      "-2.4317\n",
      "-2.44493\n",
      "-2.57777\n",
      "-2.54991\n",
      "-2.46569\n",
      "-2.55132\n",
      "-2.49032\n",
      "-2.51873\n",
      "-2.40859\n",
      "-2.58396\n",
      "-2.48849\n",
      "-2.4409\n",
      "-2.54725\n",
      "-2.51071\n",
      "-2.47018\n",
      "-2.57354\n",
      "-2.56673\n",
      "-2.57897\n",
      "-2.47381\n",
      "-2.45505\n",
      "-2.54663\n",
      "-2.51154\n",
      "-2.51188\n",
      "-2.49902\n",
      "-2.47466\n",
      "-2.43065\n",
      "-2.49188\n",
      "-2.5003\n",
      "-2.51595\n",
      "-2.46909\n",
      "-2.51351\n",
      "-2.48596\n",
      "-2.51875\n",
      "-2.57858\n",
      "-2.59356\n",
      "-2.54138\n",
      "-2.52329\n",
      "-2.49479\n",
      "-2.61295\n",
      "-2.55535\n",
      "-2.55366\n",
      "-2.54455\n",
      "-2.48476\n",
      "-2.52706\n",
      "-2.49644\n",
      "-2.53511\n",
      "-2.57596\n",
      "-2.4562\n",
      "-2.54405\n",
      "-2.49898\n",
      "-2.59648\n",
      "-2.44607\n",
      "-2.54698\n",
      "-2.46811\n",
      "-2.59563\n",
      "-2.49019\n",
      "-2.53598\n",
      "-2.52791\n",
      "-2.47751\n",
      "-2.58394\n",
      "-2.49726\n",
      "-2.51737\n",
      "-2.51843\n",
      "-2.4384\n",
      "-2.4743\n",
      "-2.573\n",
      "-2.52978\n",
      "-2.50469\n",
      "-2.47476\n",
      "-2.53196\n",
      "-2.49962\n",
      "-2.52989\n",
      "-2.50805\n",
      "-2.50821\n",
      "-2.56579\n",
      "-2.48576\n",
      "-2.63002\n",
      "-2.45619\n",
      "-2.5196\n",
      "-2.56032\n",
      "-2.41154\n",
      "-2.49705\n",
      "-2.44653\n",
      "-2.48632\n",
      "-2.51294\n",
      "-2.50189\n",
      "-2.54157\n",
      "-2.49168\n",
      "-2.52962\n",
      "-2.48325\n",
      "-2.49983\n",
      "-2.50725\n",
      "-2.52921\n",
      "-2.4764\n",
      "-2.46726\n",
      "-2.53512\n",
      "-2.64236\n",
      "-2.54641\n",
      "-2.4629\n",
      "-2.5132\n",
      "-2.51027\n",
      "-2.57692\n",
      "-2.53315\n",
      "-2.40479\n",
      "-2.59149\n",
      "-2.45162\n",
      "-2.51217\n",
      "-2.64544\n",
      "-2.56381\n",
      "-2.61031\n",
      "-2.51666\n",
      "-2.47563\n",
      "-2.53096\n",
      "-2.49777\n",
      "-2.52219\n",
      "-2.49252\n",
      "-2.48165\n",
      "-2.54389\n",
      "-2.54139\n",
      "-2.51685\n",
      "-2.45213\n",
      "-2.49894\n",
      "-2.57768\n",
      "-2.48002\n",
      "-2.39054\n",
      "-2.46885\n",
      "-2.50871\n",
      "-2.46796\n",
      "-2.61408\n",
      "-2.48945\n",
      "-2.52408\n",
      "-2.41367\n",
      "-2.58735\n",
      "-2.44691\n",
      "-2.50896\n",
      "-2.54864\n",
      "-2.51966\n",
      "-2.50759\n",
      "-2.59421\n",
      "-2.62413\n",
      "-2.52467\n",
      "-2.6067\n",
      "-2.57434\n",
      "-2.61309\n",
      "-2.4982\n",
      "-2.54979\n",
      "-2.51828\n",
      "-2.50517\n",
      "-2.53641\n",
      "-2.47309\n",
      "-2.55587\n",
      "-2.52301\n",
      "-2.48262\n",
      "-2.52747\n",
      "-2.44575\n",
      "-2.50865\n",
      "-2.60801\n",
      "-2.60448\n",
      "-2.51067\n",
      "-2.49738\n",
      "-2.58848\n",
      "-2.47767\n",
      "-2.62859\n",
      "-2.56647\n",
      "-2.59489\n",
      "-2.56821\n",
      "-2.50894\n",
      "-2.55004\n",
      "-2.44386\n",
      "-2.54268\n",
      "-2.48275\n",
      "-2.54383\n",
      "-2.58146\n",
      "-2.50754\n",
      "-2.50099\n",
      "-2.44761\n",
      "-2.47429\n",
      "-2.50266\n",
      "-2.46057\n",
      "-2.55105\n",
      "-2.51745\n",
      "-2.47062\n",
      "-2.48119\n",
      "-2.44929\n",
      "-2.48237\n",
      "-2.51666\n",
      "-2.53318\n",
      "-2.54906\n",
      "-2.54372\n",
      "-2.60328\n",
      "-2.52253\n",
      "-2.5205\n",
      "-2.49169\n",
      "-2.49252\n",
      "-2.55647\n",
      "-2.42604\n",
      "-2.4688\n",
      "-2.53732\n",
      "-2.45762\n",
      "-2.55332\n",
      "-2.52289\n",
      "-2.52044\n",
      "-2.45445\n",
      "-2.48812\n",
      "-2.4392\n",
      "-2.41965\n",
      "-2.48926\n",
      "-2.45336\n",
      "-2.47642\n",
      "-2.48238\n",
      "-2.46343\n",
      "-2.55915\n",
      "-2.50767\n",
      "-2.50462\n",
      "-2.51641\n",
      "-2.52957\n",
      "-2.50115\n",
      "-2.51518\n",
      "-2.4676\n",
      "-2.50173\n",
      "-2.39024\n",
      "-2.56971\n",
      "-2.43134\n",
      "-2.44631\n",
      "-2.54621\n",
      "-2.4161\n",
      "-2.41342\n",
      "-2.4092\n",
      "-2.50692\n",
      "-2.41853\n",
      "-2.4894\n",
      "-2.50948\n",
      "-2.52449\n",
      "-2.50494\n",
      "-2.50553\n",
      "-2.5338\n",
      "-2.44125\n",
      "-2.57625\n",
      "-2.43653\n",
      "-2.48192\n",
      "-2.47906\n",
      "-2.49727\n",
      "-2.44686\n",
      "-2.42807\n",
      "-2.56019\n",
      "-2.4795\n",
      "-2.53115\n",
      "-2.53538\n",
      "-2.5457\n",
      "-2.54603\n",
      "-2.58461\n",
      "-2.54567\n",
      "-2.50555\n",
      "-2.53032\n",
      "-2.47412\n",
      "-2.48334\n",
      "-2.4715\n",
      "-2.42887\n",
      "-2.61665\n",
      "-2.51443\n",
      "-2.50523\n",
      "-2.4752\n",
      "-2.49298\n",
      "-2.47874\n",
      "-2.58281\n",
      "-2.45315\n",
      "-2.57977\n",
      "-2.4219\n",
      "-2.51496\n",
      "-2.46647\n",
      "-2.46501\n",
      "-2.55262\n",
      "-2.48536\n",
      "-2.60152\n",
      "-2.49344\n",
      "-2.48017\n",
      "-2.51574\n",
      "-2.49817\n",
      "-2.52191\n",
      "-2.47898\n",
      "-2.47122\n",
      "-2.47827\n",
      "-2.60031\n",
      "-2.57611\n",
      "-2.44266\n",
      "-2.54484\n",
      "-2.57685\n",
      "-2.50713\n",
      "-2.47963\n",
      "-2.40128\n",
      "-2.52811\n",
      "-2.53281\n",
      "-2.42198\n",
      "-2.47032\n",
      "-2.49368\n",
      "-2.49106\n",
      "-2.40997\n",
      "-2.47816\n",
      "-2.53861\n",
      "-2.56346\n",
      "-2.42053\n",
      "-2.49552\n",
      "-2.43463\n",
      "-2.4532\n",
      "-2.54069\n",
      "-2.5065\n",
      "-2.47369\n",
      "-2.51249\n",
      "-2.54963\n",
      "-2.5441\n",
      "-2.49928\n",
      "-2.52656\n",
      "-2.51073\n",
      "-2.55321\n",
      "-2.52063\n",
      "-2.45083\n",
      "-2.4599\n",
      "-2.4825\n",
      "-2.50126\n",
      "-2.51325\n",
      "-2.45349\n",
      "-2.57013\n",
      "-2.56323\n",
      "-2.45316\n",
      "-2.53851\n",
      "-2.46741\n",
      "-2.52783\n",
      "-2.49339\n",
      "-2.52101\n",
      "-2.50931\n",
      "-2.45807\n",
      "-2.52561\n",
      "-2.61854\n",
      "-2.51714\n",
      "-2.53704\n",
      "-2.45352\n",
      "-2.42204\n",
      "-2.48372\n",
      "-2.51529\n",
      "-2.47895\n",
      "-2.52436\n",
      "-2.43771\n",
      "-2.50716\n",
      "-2.52909\n",
      "-2.41573\n",
      "-2.48435\n",
      "-2.44296\n",
      "-2.47902\n",
      "-2.45885\n",
      "-2.50047\n",
      "-2.58137\n",
      "-2.53888\n",
      "-2.54911\n",
      "-2.51017\n",
      "-2.50116\n",
      "-2.56743\n",
      "-2.49732\n",
      "-2.53528\n",
      "-2.49041\n",
      "-2.49878\n",
      "-2.45915\n",
      "-2.52492\n",
      "-2.55953\n",
      "-2.40573\n",
      "-2.51802\n",
      "-2.41727\n",
      "-2.49188\n",
      "-2.45057\n",
      "-2.49693\n",
      "-2.46005\n",
      "-2.48641\n",
      "-2.40196\n",
      "-2.55772\n",
      "-2.51662\n",
      "-2.38441\n",
      "-2.4726\n",
      "-2.47727\n",
      "-2.57042\n",
      "-2.59354\n",
      "-2.52537\n",
      "-2.44118\n",
      "-2.56934\n",
      "-2.45436\n",
      "-2.54363\n",
      "-2.46021\n",
      "-2.48855\n",
      "-2.47504\n",
      "-2.37579\n",
      "-2.51752\n",
      "-2.45294\n",
      "-2.54062\n",
      "-2.5643\n",
      "-2.53417\n",
      "-2.54399\n",
      "-2.57349\n",
      "-2.47041\n",
      "-2.45099\n",
      "-2.46433\n",
      "-2.44659\n",
      "-2.51091\n",
      "-2.53197\n",
      "-2.46738\n",
      "-2.47693\n",
      "-2.47294\n",
      "-2.45233\n",
      "-2.52834\n",
      "-2.5202\n",
      "-2.49136\n",
      "-2.45533\n",
      "-2.47129\n",
      "-2.49495\n",
      "-2.45506\n",
      "-2.42919\n",
      "-2.52424\n",
      "-2.47731\n",
      "-2.5286\n",
      "-2.43495\n",
      "-2.5382\n",
      "-2.48114\n",
      "-2.49016\n",
      "-2.42402\n",
      "-2.48334\n",
      "-2.42333\n",
      "-2.47446\n",
      "-2.56738\n",
      "-2.41358\n",
      "-2.53476\n",
      "-2.42838\n",
      "-2.57077\n",
      "-2.57939\n",
      "-2.5192\n",
      "-2.50583\n",
      "-2.44352\n",
      "-2.48512\n",
      "-2.5901\n",
      "-2.52397\n",
      "-2.49413\n",
      "-2.47708\n",
      "-2.49978\n",
      "-2.41943\n",
      "-2.57802\n",
      "-2.53249\n",
      "-2.524\n",
      "-2.49355\n",
      "-2.52114\n",
      "-2.41632\n",
      "-2.42925\n",
      "-2.49876\n",
      "-2.56209\n",
      "-2.41038\n",
      "-2.54132\n",
      "-2.55661\n",
      "-2.47111\n",
      "-2.51062\n",
      "-2.4518\n",
      "-2.52136\n",
      "-2.49475\n",
      "-2.40804\n",
      "-2.46406\n",
      "-2.50002\n",
      "-2.52562\n",
      "-2.4771\n",
      "-2.48307\n",
      "-2.52613\n",
      "-2.52314\n",
      "-2.36224\n",
      "-2.38558\n",
      "-2.5015\n",
      "-2.49331\n",
      "-2.5195\n",
      "-2.46287\n",
      "-2.41965\n",
      "-2.48504\n",
      "-2.41446\n",
      "-2.49318\n",
      "-2.46491\n",
      "-2.34515\n",
      "-2.42537\n",
      "-2.51811\n",
      "-2.44652\n",
      "-2.50863\n",
      "-2.45817\n",
      "-2.59431\n",
      "-2.53886\n",
      "-2.55268\n",
      "-2.39112\n",
      "-2.38842\n",
      "-2.47972\n",
      "-2.46242\n",
      "-2.41563\n",
      "-2.48073\n",
      "-2.52971\n",
      "-2.4533\n",
      "-2.44062\n",
      "-2.434\n",
      "-2.47686\n",
      "-2.54731\n",
      "-2.48827\n",
      "-2.49647\n",
      "-2.49102\n",
      "-2.44199\n",
      "-2.47279\n",
      "-2.4939\n",
      "-2.4508\n",
      "-2.46351\n",
      "-2.42601\n",
      "-2.51144\n",
      "-2.5402\n",
      "-2.50693\n",
      "-2.47959\n",
      "-2.44937\n",
      "-2.4563\n",
      "-2.45729\n",
      "-2.44766\n",
      "-2.49554\n",
      "-2.51004\n",
      "-2.44195\n",
      "-2.47822\n",
      "-2.52004\n",
      "-2.55201\n",
      "-2.42353\n",
      "-2.41123\n",
      "-2.54015\n",
      "-2.45972\n",
      "-2.46867\n",
      "-2.46987\n",
      "-2.53362\n",
      "-2.55467\n",
      "-2.40779\n",
      "-2.54824\n",
      "-2.45355\n",
      "-2.42557\n",
      "-2.48602\n",
      "-2.52032\n",
      "-2.49451\n",
      "-2.44977\n",
      "-2.47695\n",
      "-2.51101\n",
      "-2.52477\n",
      "-2.53056\n",
      "-2.43517\n",
      "-2.47508\n",
      "-2.46212\n",
      "-2.384\n",
      "-2.56443\n"
     ]
    }
   ],
   "source": [
    "learn_loc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_data[:,-1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
